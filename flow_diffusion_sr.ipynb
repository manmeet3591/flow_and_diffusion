{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "ds_dmsp = xr.open_dataset('/mnt/dmsp_2012_austin.nc').constant\n",
    "ds_viirs = xr.open_dataset('/mnt/viirs_2012_austin.nc').avg_rad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = ds_viirs.values.astype(np.float32)\n",
    "y_train = ds_dmsp.values[0,:,:].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unpatchify(patches, img_shape):\n",
    "    patch_size = patches.shape[1]\n",
    "    assert patches.shape[0] == (img_shape[0] // patch_size) * (img_shape[1] // patch_size), \"Patches and image shape are not compatible\"\n",
    "\n",
    "    img = np.zeros(img_shape, dtype=patches.dtype)\n",
    "    patch_idx = 0\n",
    "\n",
    "    for i in range(0, img_shape[0], patch_size):\n",
    "        for j in range(0, img_shape[1], patch_size):\n",
    "            img[i:i + patch_size, j:j + patch_size] = patches[patch_idx]\n",
    "            patch_idx += 1\n",
    "\n",
    "    return img\n",
    "\n",
    "def patchify(img, patch_size):\n",
    "    img_shape = img.shape\n",
    "    patches = np.array([img[i:i + patch_size, j:j + patch_size] for i in range(0, img_shape[0], patch_size) for j in range(0, img_shape[1], patch_size)])\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "class ncDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = torch.from_numpy(self.data[index]).unsqueeze(0)\n",
    "        y = torch.from_numpy(self.targets[index]).unsqueeze(0)\n",
    "        # x = self.data[index]\n",
    "        # y = self.targets[index]\n",
    "        # x = x.to(dtype=torch.float32)\n",
    "        # y = y.to(dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        lr, hr = batch\n",
    "        lr, hr = lr.to(device), hr.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        sr = model(lr)\n",
    "        loss = criterion(sr, hr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            lr, hr = batch\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "            sr = model(lr)\n",
    "            loss = criterion(sr, hr)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create patches from the image\n",
    "patch_size = 32\n",
    "img = x_train[:576,:576]\n",
    "patches = patchify(img, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the image from the patches\n",
    "reconstructed_img = unpatchify(patches, img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the reconstructed image is the same as the original image\n",
    "assert np.allclose(reconstructed_img, img), \"The reconstructed image is not the same as the original image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(324, 32, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_max = x_train.max()\n",
    "y_train_max = y_train.max()\n",
    "x_train /= x_train_max\n",
    "y_train /= y_train_max\n",
    "\n",
    "x_train_patches = patchify(x_train[:576,:576], patch_size)\n",
    "y_train_patches = patchify(y_train[:576,:576], patch_size)\n",
    "x_train_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_patches = x_train_patches[200:300]\n",
    "y_val_patches = y_train_patches[200:300]\n",
    "\n",
    "x_test_patches = x_train_patches[300:]\n",
    "y_test_patches = y_train_patches[300:]\n",
    "\n",
    "x_train_patches = x_train_patches[:200]\n",
    "\n",
    "y_train_patches = y_train_patches[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 1, kernel_size=5, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ncDataset(x_train_patches, y_train_patches)\n",
    "val_dataset = ncDataset(x_val_patches, y_val_patches)\n",
    "test_dataset = ncDataset(x_val_patches, y_val_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32]) torch.Size([1, 32, 32]) 200\n"
     ]
    }
   ],
   "source": [
    "lr, hr = train_dataset.__getitem__(0)\n",
    "print(lr.shape, hr.shape, train_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=20, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 32, 32])\n",
      "torch.Size([20, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    data, targets = batch\n",
    "    print(data.size())  # Should print torch.Size([16, 1, 30, 30])\n",
    "    print(targets.size())  # Should print torch.Size([16, 1, 30, 601])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "device = 'cuda'\n",
    "model = SRCNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000] - Train Loss: 0.0016, Validation Loss: 0.0096\n",
      "Epoch [20/1000] - Train Loss: 0.0016, Validation Loss: 0.0098\n",
      "Epoch [30/1000] - Train Loss: 0.0015, Validation Loss: 0.0085\n",
      "Epoch [40/1000] - Train Loss: 0.0016, Validation Loss: 0.0085\n",
      "Epoch [50/1000] - Train Loss: 0.0015, Validation Loss: 0.0081\n",
      "Epoch [60/1000] - Train Loss: 0.0017, Validation Loss: 0.0083\n",
      "Epoch [70/1000] - Train Loss: 0.0016, Validation Loss: 0.0080\n",
      "Epoch [80/1000] - Train Loss: 0.0015, Validation Loss: 0.0089\n",
      "Epoch [90/1000] - Train Loss: 0.0016, Validation Loss: 0.0087\n",
      "Epoch [100/1000] - Train Loss: 0.0015, Validation Loss: 0.0103\n",
      "Epoch [110/1000] - Train Loss: 0.0015, Validation Loss: 0.0089\n",
      "Epoch [120/1000] - Train Loss: 0.0015, Validation Loss: 0.0077\n",
      "Epoch [130/1000] - Train Loss: 0.0014, Validation Loss: 0.0082\n",
      "Epoch [140/1000] - Train Loss: 0.0014, Validation Loss: 0.0087\n",
      "Epoch [150/1000] - Train Loss: 0.0016, Validation Loss: 0.0091\n",
      "Epoch [160/1000] - Train Loss: 0.0014, Validation Loss: 0.0102\n",
      "Epoch [170/1000] - Train Loss: 0.0014, Validation Loss: 0.0084\n",
      "Epoch [180/1000] - Train Loss: 0.0014, Validation Loss: 0.0076\n",
      "Epoch [190/1000] - Train Loss: 0.0015, Validation Loss: 0.0084\n",
      "Epoch [200/1000] - Train Loss: 0.0014, Validation Loss: 0.0070\n",
      "Epoch [210/1000] - Train Loss: 0.0013, Validation Loss: 0.0084\n",
      "Epoch [220/1000] - Train Loss: 0.0015, Validation Loss: 0.0085\n",
      "Epoch [230/1000] - Train Loss: 0.0014, Validation Loss: 0.0073\n",
      "Epoch [240/1000] - Train Loss: 0.0014, Validation Loss: 0.0086\n",
      "Epoch [250/1000] - Train Loss: 0.0013, Validation Loss: 0.0082\n",
      "Epoch [260/1000] - Train Loss: 0.0013, Validation Loss: 0.0081\n",
      "Epoch [270/1000] - Train Loss: 0.0014, Validation Loss: 0.0094\n",
      "Epoch [280/1000] - Train Loss: 0.0013, Validation Loss: 0.0099\n",
      "Epoch [290/1000] - Train Loss: 0.0014, Validation Loss: 0.0079\n",
      "Epoch [300/1000] - Train Loss: 0.0013, Validation Loss: 0.0081\n",
      "Epoch [310/1000] - Train Loss: 0.0014, Validation Loss: 0.0089\n",
      "Epoch [320/1000] - Train Loss: 0.0014, Validation Loss: 0.0085\n",
      "Epoch [330/1000] - Train Loss: 0.0013, Validation Loss: 0.0076\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "is_train = True\n",
    "num_epochs = 1000\n",
    "print_interval = 10\n",
    "patience = 50\n",
    "best_val_loss = float('inf')\n",
    "counter = 0\n",
    "best_model = None\n",
    "\n",
    "if is_train:\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, val_loss = train(model, train_dataloader, val_dataloader, criterion, optimizer, device)\n",
    "    # Log losses to TensorBoard\n",
    "        #writer.add_scalars(\"Loss\", {\"Train\": train_loss, \"Validation\": val_loss}, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = deepcopy(model)\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if epoch % print_interval == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "    #writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import UNet2DModel, EDMEulerScheduler\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "# from datasets import load_dataset\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2022, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
    "#\n",
    "# This work is licensed under a Creative Commons\n",
    "# Attribution-NonCommercial-ShareAlike 4.0 International License.\n",
    "# You should have received a copy of the license along with this\n",
    "# work. If not, see http://creativecommons.org/licenses/by-nc-sa/4.0/\n",
    "\n",
    "\"\"\"Model architectures and preconditioning schemes used in the paper\n",
    "\"Elucidating the Design Space of Diffusion-Based Generative Models\".\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_utils import persistence\n",
    "from torch.nn.functional import silu\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Unified routine for initializing weights and biases.\n",
    "\n",
    "def weight_init(shape, mode, fan_in, fan_out):\n",
    "    if mode == 'xavier_uniform': return np.sqrt(6 / (fan_in + fan_out)) * (torch.rand(*shape) * 2 - 1)\n",
    "    if mode == 'xavier_normal':  return np.sqrt(2 / (fan_in + fan_out)) * torch.randn(*shape)\n",
    "    if mode == 'kaiming_uniform': return np.sqrt(3 / fan_in) * (torch.rand(*shape) * 2 - 1)\n",
    "    if mode == 'kaiming_normal':  return np.sqrt(1 / fan_in) * torch.randn(*shape)\n",
    "    raise ValueError(f'Invalid init mode \"{mode}\"')\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Fully-connected layer.\n",
    "\n",
    "# @persistence.persistent_class\n",
    "class Linear(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, init_mode='kaiming_normal', init_weight=1, init_bias=0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        init_kwargs = dict(mode=init_mode, fan_in=in_features, fan_out=out_features)\n",
    "        self.weight = torch.nn.Parameter(weight_init([out_features, in_features], **init_kwargs) * init_weight)\n",
    "        self.bias = torch.nn.Parameter(weight_init([out_features], **init_kwargs) * init_bias) if bias else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x @ self.weight.to(x.dtype).t()\n",
    "        if self.bias is not None:\n",
    "            x = x.add_(self.bias.to(x.dtype))\n",
    "        return x\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Convolutional layer with optional up/downsampling.\n",
    "\n",
    "# @persistence.persistent_class\n",
    "class Conv2d(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels, out_channels, kernel, bias=True, up=False, down=False,\n",
    "        resample_filter=[1,1], fused_resample=False, init_mode='kaiming_normal', init_weight=1, init_bias=0,\n",
    "    ):\n",
    "        assert not (up and down)\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.up = up\n",
    "        self.down = down\n",
    "        self.fused_resample = fused_resample\n",
    "        init_kwargs = dict(mode=init_mode, fan_in=in_channels*kernel*kernel, fan_out=out_channels*kernel*kernel)\n",
    "        self.weight = torch.nn.Parameter(weight_init([out_channels, in_channels, kernel, kernel], **init_kwargs) * init_weight) if kernel else None\n",
    "        self.bias = torch.nn.Parameter(weight_init([out_channels], **init_kwargs) * init_bias) if kernel and bias else None\n",
    "        f = torch.as_tensor(resample_filter, dtype=torch.float32)\n",
    "        f = f.ger(f).unsqueeze(0).unsqueeze(1) / f.sum().square()\n",
    "        self.register_buffer('resample_filter', f if up or down else None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.weight.to(x.dtype) if self.weight is not None else None\n",
    "        b = self.bias.to(x.dtype) if self.bias is not None else None\n",
    "        f = self.resample_filter.to(x.dtype) if self.resample_filter is not None else None\n",
    "        w_pad = w.shape[-1] // 2 if w is not None else 0\n",
    "        f_pad = (f.shape[-1] - 1) // 2 if f is not None else 0\n",
    "\n",
    "        if self.fused_resample and self.up and w is not None:\n",
    "            x = torch.nn.functional.conv_transpose2d(x, f.mul(4).tile([self.in_channels, 1, 1, 1]), groups=self.in_channels, stride=2, padding=max(f_pad - w_pad, 0))\n",
    "            x = torch.nn.functional.conv2d(x, w, padding=max(w_pad - f_pad, 0))\n",
    "        elif self.fused_resample and self.down and w is not None:\n",
    "            x = torch.nn.functional.conv2d(x, w, padding=w_pad+f_pad)\n",
    "            x = torch.nn.functional.conv2d(x, f.tile([self.out_channels, 1, 1, 1]), groups=self.out_channels, stride=2)\n",
    "        else:\n",
    "            if self.up:\n",
    "                x = torch.nn.functional.conv_transpose2d(x, f.mul(4).tile([self.in_channels, 1, 1, 1]), groups=self.in_channels, stride=2, padding=f_pad)\n",
    "            if self.down:\n",
    "                x = torch.nn.functional.conv2d(x, f.tile([self.in_channels, 1, 1, 1]), groups=self.in_channels, stride=2, padding=f_pad)\n",
    "            if w is not None:\n",
    "                x = torch.nn.functional.conv2d(x, w, padding=w_pad)\n",
    "        if b is not None:\n",
    "            x = x.add_(b.reshape(1, -1, 1, 1))\n",
    "        return x\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Group normalization.\n",
    "\n",
    "# @persistence.persistent_class\n",
    "class GroupNorm(torch.nn.Module):\n",
    "    def __init__(self, num_channels, num_groups=32, min_channels_per_group=4, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.num_groups = min(num_groups, num_channels // min_channels_per_group)\n",
    "        self.eps = eps\n",
    "        self.weight = torch.nn.Parameter(torch.ones(num_channels))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(num_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.group_norm(x, num_groups=self.num_groups, weight=self.weight.to(x.dtype), bias=self.bias.to(x.dtype), eps=self.eps)\n",
    "        return x\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Attention weight computation, i.e., softmax(Q^T * K).\n",
    "# Performs all computation using FP32, but uses the original datatype for\n",
    "# inputs/outputs/gradients to conserve memory.\n",
    "\n",
    "class AttentionOp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, q, k):\n",
    "        w = torch.einsum('ncq,nck->nqk', q.to(torch.float32), (k / np.sqrt(k.shape[1])).to(torch.float32)).softmax(dim=2).to(q.dtype)\n",
    "        ctx.save_for_backward(q, k, w)\n",
    "        return w\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dw):\n",
    "        q, k, w = ctx.saved_tensors\n",
    "        db = torch._softmax_backward_data(grad_output=dw.to(torch.float32), output=w.to(torch.float32), dim=2, input_dtype=torch.float32)\n",
    "        dq = torch.einsum('nck,nqk->ncq', k.to(torch.float32), db).to(q.dtype) / np.sqrt(k.shape[1])\n",
    "        dk = torch.einsum('ncq,nqk->nck', q.to(torch.float32), db).to(k.dtype) / np.sqrt(k.shape[1])\n",
    "        return dq, dk\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Unified U-Net block with optional up/downsampling and self-attention.\n",
    "# Represents the union of all features employed by the DDPM++, NCSN++, and\n",
    "# ADM architectures.\n",
    "\n",
    "# @persistence.persistent_class\n",
    "class UNetBlock(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        in_channels, out_channels, emb_channels, up=False, down=False, attention=False,\n",
    "        num_heads=None, channels_per_head=64, dropout=0, skip_scale=1, eps=1e-5,\n",
    "        resample_filter=[1,1], resample_proj=False, adaptive_scale=True,\n",
    "        init=dict(), init_zero=dict(init_weight=0), init_attn=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.emb_channels = emb_channels\n",
    "        self.num_heads = 0 if not attention else num_heads if num_heads is not None else out_channels // channels_per_head\n",
    "        self.dropout = dropout\n",
    "        self.skip_scale = skip_scale\n",
    "        self.adaptive_scale = adaptive_scale\n",
    "\n",
    "        self.norm0 = GroupNorm(num_channels=in_channels, eps=eps)\n",
    "        self.conv0 = Conv2d(in_channels=in_channels, out_channels=out_channels, kernel=3, up=up, down=down, resample_filter=resample_filter, **init)\n",
    "        self.affine = Linear(in_features=emb_channels, out_features=out_channels*(2 if adaptive_scale else 1), **init)\n",
    "        self.norm1 = GroupNorm(num_channels=out_channels, eps=eps)\n",
    "        self.conv1 = Conv2d(in_channels=out_channels, out_channels=out_channels, kernel=3, **init_zero)\n",
    "\n",
    "        self.skip = None\n",
    "        if out_channels != in_channels or up or down:\n",
    "            kernel = 1 if resample_proj or out_channels!= in_channels else 0\n",
    "            self.skip = Conv2d(in_channels=in_channels, out_channels=out_channels, kernel=kernel, up=up, down=down, resample_filter=resample_filter, **init)\n",
    "\n",
    "        if self.num_heads:\n",
    "            self.norm2 = GroupNorm(num_channels=out_channels, eps=eps)\n",
    "            self.qkv = Conv2d(in_channels=out_channels, out_channels=out_channels*3, kernel=1, **(init_attn if init_attn is not None else init))\n",
    "            self.proj = Conv2d(in_channels=out_channels, out_channels=out_channels, kernel=1, **init_zero)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        orig = x\n",
    "        x = self.conv0(silu(self.norm0(x)))\n",
    "\n",
    "        params = self.affine(emb).unsqueeze(2).unsqueeze(3).to(x.dtype)\n",
    "        if self.adaptive_scale:\n",
    "            scale, shift = params.chunk(chunks=2, dim=1)\n",
    "            x = silu(torch.addcmul(shift, self.norm1(x), scale + 1))\n",
    "        else:\n",
    "            x = silu(self.norm1(x.add_(params)))\n",
    "\n",
    "        x = self.conv1(torch.nn.functional.dropout(x, p=self.dropout, training=self.training))\n",
    "        x = x.add_(self.skip(orig) if self.skip is not None else orig)\n",
    "        x = x * self.skip_scale\n",
    "\n",
    "        if self.num_heads:\n",
    "            q, k, v = self.qkv(self.norm2(x)).reshape(x.shape[0] * self.num_heads, x.shape[1] // self.num_heads, 3, -1).unbind(2)\n",
    "            w = AttentionOp.apply(q, k)\n",
    "            a = torch.einsum('nqk,nck->ncq', w, v)\n",
    "            x = self.proj(a.reshape(*x.shape)).add_(x)\n",
    "            x = x * self.skip_scale\n",
    "        return x\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Timestep embedding used in the DDPM++ and ADM architectures.\n",
    "\n",
    "# @persistence.persistent_class\n",
    "class PositionalEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_channels, max_positions=10000, endpoint=False):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.max_positions = max_positions\n",
    "        self.endpoint = endpoint\n",
    "\n",
    "    def forward(self, x):\n",
    "        freqs = torch.arange(start=0, end=self.num_channels//2, dtype=torch.float32, device=x.device)\n",
    "        freqs = freqs / (self.num_channels // 2 - (1 if self.endpoint else 0))\n",
    "        freqs = (1 / self.max_positions) ** freqs\n",
    "        x = x.ger(freqs.to(x.dtype))\n",
    "        x = torch.cat([x.cos(), x.sin()], dim=1)\n",
    "        return x\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Timestep embedding used in the NCSN++ architecture.\n",
    "\n",
    "# @persistence.persistent_class\n",
    "class FourierEmbedding(torch.nn.Module):\n",
    "    def __init__(self, num_channels, scale=16):\n",
    "        super().__init__()\n",
    "        self.register_buffer('freqs', torch.randn(num_channels // 2) * scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.ger((2 * np.pi * self.freqs).to(x.dtype))\n",
    "        x = torch.cat([x.cos(), x.sin()], dim=1)\n",
    "        return x\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Reimplementation of the DDPM++ and NCSN++ architectures from the paper\n",
    "# \"Score-Based Generative Modeling through Stochastic Differential\n",
    "# Equations\". Equivalent to the original implementation by Song et al.,\n",
    "# available at https://github.com/yang-song/score_sde_pytorch\n",
    "\n",
    "# @persistence.persistent_class\n",
    "class SongUNet(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        img_resolution,                     # Image resolution at input/output.\n",
    "        in_channels,                        # Number of color channels at input.\n",
    "        out_channels,                       # Number of color channels at output.\n",
    "        label_dim           = 0,            # Number of class labels, 0 = unconditional.\n",
    "        augment_dim         = 0,            # Augmentation label dimensionality, 0 = no augmentation.\n",
    "\n",
    "        model_channels      = 128,          # Base multiplier for the number of channels.\n",
    "        channel_mult        = [1,2,2,2],    # Per-resolution multipliers for the number of channels.\n",
    "        channel_mult_emb    = 4,            # Multiplier for the dimensionality of the embedding vector.\n",
    "        num_blocks          = 4,            # Number of residual blocks per resolution.\n",
    "        attn_resolutions    = [16],         # List of resolutions with self-attention.\n",
    "        dropout             = 0.10,         # Dropout probability of intermediate activations.\n",
    "        label_dropout       = 0,            # Dropout probability of class labels for classifier-free guidance.\n",
    "\n",
    "        embedding_type      = 'positional', # Timestep embedding type: 'positional' for DDPM++, 'fourier' for NCSN++.\n",
    "        channel_mult_noise  = 1,            # Timestep embedding size: 1 for DDPM++, 2 for NCSN++.\n",
    "        encoder_type        = 'standard',   # Encoder architecture: 'standard' for DDPM++, 'residual' for NCSN++.\n",
    "        decoder_type        = 'standard',   # Decoder architecture: 'standard' for both DDPM++ and NCSN++.\n",
    "        resample_filter     = [1,1],        # Resampling filter: [1,1] for DDPM++, [1,3,3,1] for NCSN++.\n",
    "    ):\n",
    "        assert embedding_type in ['fourier', 'positional']\n",
    "        assert encoder_type in ['standard', 'skip', 'residual']\n",
    "        assert decoder_type in ['standard', 'skip']\n",
    "\n",
    "        super().__init__()\n",
    "        self.label_dropout = label_dropout\n",
    "        emb_channels = model_channels * channel_mult_emb\n",
    "        noise_channels = model_channels * channel_mult_noise\n",
    "        init = dict(init_mode='xavier_uniform')\n",
    "        init_zero = dict(init_mode='xavier_uniform', init_weight=1e-5)\n",
    "        init_attn = dict(init_mode='xavier_uniform', init_weight=np.sqrt(0.2))\n",
    "        block_kwargs = dict(\n",
    "            emb_channels=emb_channels, num_heads=1, dropout=dropout, skip_scale=np.sqrt(0.5), eps=1e-6,\n",
    "            resample_filter=resample_filter, resample_proj=True, adaptive_scale=False,\n",
    "            init=init, init_zero=init_zero, init_attn=init_attn,\n",
    "        )\n",
    "\n",
    "        # Mapping.\n",
    "        self.map_noise = PositionalEmbedding(num_channels=noise_channels, endpoint=True) if embedding_type == 'positional' else FourierEmbedding(num_channels=noise_channels)\n",
    "        self.map_label = Linear(in_features=label_dim, out_features=noise_channels, **init) if label_dim else None\n",
    "        self.map_augment = Linear(in_features=augment_dim, out_features=noise_channels, bias=False, **init) if augment_dim else None\n",
    "        self.map_layer0 = Linear(in_features=noise_channels, out_features=emb_channels, **init)\n",
    "        self.map_layer1 = Linear(in_features=emb_channels, out_features=emb_channels, **init)\n",
    "\n",
    "        # Encoder.\n",
    "        self.enc = torch.nn.ModuleDict()\n",
    "        cout = in_channels\n",
    "        caux = in_channels\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            res = img_resolution >> level\n",
    "            if level == 0:\n",
    "                cin = cout\n",
    "                cout = model_channels\n",
    "                self.enc[f'{res}x{res}_conv'] = Conv2d(in_channels=cin, out_channels=cout, kernel=3, **init)\n",
    "            else:\n",
    "                self.enc[f'{res}x{res}_down'] = UNetBlock(in_channels=cout, out_channels=cout, down=True, **block_kwargs)\n",
    "                if encoder_type == 'skip':\n",
    "                    self.enc[f'{res}x{res}_aux_down'] = Conv2d(in_channels=caux, out_channels=caux, kernel=0, down=True, resample_filter=resample_filter)\n",
    "                    self.enc[f'{res}x{res}_aux_skip'] = Conv2d(in_channels=caux, out_channels=cout, kernel=1, **init)\n",
    "                if encoder_type == 'residual':\n",
    "                    self.enc[f'{res}x{res}_aux_residual'] = Conv2d(in_channels=caux, out_channels=cout, kernel=3, down=True, resample_filter=resample_filter, fused_resample=True, **init)\n",
    "                    caux = cout\n",
    "            for idx in range(num_blocks):\n",
    "                cin = cout\n",
    "                cout = model_channels * mult\n",
    "                attn = (res in attn_resolutions)\n",
    "                self.enc[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=attn, **block_kwargs)\n",
    "        skips = [block.out_channels for name, block in self.enc.items() if 'aux' not in name]\n",
    "\n",
    "        # Decoder.\n",
    "        self.dec = torch.nn.ModuleDict()\n",
    "        for level, mult in reversed(list(enumerate(channel_mult))):\n",
    "            res = img_resolution >> level\n",
    "            if level == len(channel_mult) - 1:\n",
    "                self.dec[f'{res}x{res}_in0'] = UNetBlock(in_channels=cout, out_channels=cout, attention=True, **block_kwargs)\n",
    "                self.dec[f'{res}x{res}_in1'] = UNetBlock(in_channels=cout, out_channels=cout, **block_kwargs)\n",
    "            else:\n",
    "                self.dec[f'{res}x{res}_up'] = UNetBlock(in_channels=cout, out_channels=cout, up=True, **block_kwargs)\n",
    "            for idx in range(num_blocks + 1):\n",
    "                cin = cout + skips.pop()\n",
    "                cout = model_channels * mult\n",
    "                attn = (idx == num_blocks and res in attn_resolutions)\n",
    "                self.dec[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=attn, **block_kwargs)\n",
    "            if decoder_type == 'skip' or level == 0:\n",
    "                if decoder_type == 'skip' and level < len(channel_mult) - 1:\n",
    "                    self.dec[f'{res}x{res}_aux_up'] = Conv2d(in_channels=out_channels, out_channels=out_channels, kernel=0, up=True, resample_filter=resample_filter)\n",
    "                self.dec[f'{res}x{res}_aux_norm'] = GroupNorm(num_channels=cout, eps=1e-6)\n",
    "                self.dec[f'{res}x{res}_aux_conv'] = Conv2d(in_channels=cout, out_channels=out_channels, kernel=3, **init_zero)\n",
    "\n",
    "    def forward(self, x, noise_labels, class_labels, augment_labels=None):\n",
    "        # Mapping.\n",
    "        emb = self.map_noise(noise_labels)\n",
    "        emb = emb.reshape(emb.shape[0], 2, -1).flip(1).reshape(*emb.shape) # swap sin/cos\n",
    "        if self.map_label is not None:\n",
    "            tmp = class_labels\n",
    "            if self.training and self.label_dropout:\n",
    "                tmp = tmp * (torch.rand([x.shape[0], 1], device=x.device) >= self.label_dropout).to(tmp.dtype)\n",
    "            emb = emb + self.map_label(tmp * np.sqrt(self.map_label.in_features))\n",
    "        if self.map_augment is not None and augment_labels is not None:\n",
    "            emb = emb + self.map_augment(augment_labels)\n",
    "        emb = silu(self.map_layer0(emb))\n",
    "        emb = silu(self.map_layer1(emb))\n",
    "\n",
    "        # Encoder.\n",
    "        skips = []\n",
    "        aux = x\n",
    "        for name, block in self.enc.items():\n",
    "            if 'aux_down' in name:\n",
    "                aux = block(aux)\n",
    "            elif 'aux_skip' in name:\n",
    "                x = skips[-1] = x + block(aux)\n",
    "            elif 'aux_residual' in name:\n",
    "                x = skips[-1] = aux = (x + block(aux)) / np.sqrt(2)\n",
    "            else:\n",
    "                x = block(x, emb) if isinstance(block, UNetBlock) else block(x)\n",
    "                skips.append(x)\n",
    "\n",
    "        # Decoder.\n",
    "        aux = None\n",
    "        tmp = None\n",
    "        for name, block in self.dec.items():\n",
    "            if 'aux_up' in name:\n",
    "                aux = block(aux)\n",
    "            elif 'aux_norm' in name:\n",
    "                tmp = block(x)\n",
    "            elif 'aux_conv' in name:\n",
    "                tmp = block(silu(tmp))\n",
    "                aux = tmp if aux is None else tmp + aux\n",
    "            else:\n",
    "                if x.shape[1] != block.in_channels:\n",
    "                    x = torch.cat([x, skips.pop()], dim=1)\n",
    "                x = block(x, emb)\n",
    "        return aux\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Reimplementation of the ADM architecture from the paper\n",
    "# \"Diffusion Models Beat GANS on Image Synthesis\". Equivalent to the\n",
    "# original implementation by Dhariwal and Nichol, available at\n",
    "# https://github.com/openai/guided-diffusion\n",
    "\n",
    "# @persistence.persistent_class\n",
    "class DhariwalUNet(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        img_resolution,                     # Image resolution at input/output.\n",
    "        in_channels,                        # Number of color channels at input.\n",
    "        out_channels,                       # Number of color channels at output.\n",
    "        label_dim           = 0,            # Number of class labels, 0 = unconditional.\n",
    "        augment_dim         = 0,            # Augmentation label dimensionality, 0 = no augmentation.\n",
    "\n",
    "        model_channels      = 192,          # Base multiplier for the number of channels.\n",
    "        channel_mult        = [1,2,3,4],    # Per-resolution multipliers for the number of channels.\n",
    "        channel_mult_emb    = 4,            # Multiplier for the dimensionality of the embedding vector.\n",
    "        num_blocks          = 3,            # Number of residual blocks per resolution.\n",
    "        attn_resolutions    = [32,16,8],    # List of resolutions with self-attention.\n",
    "        dropout             = 0.10,         # List of resolutions with self-attention.\n",
    "        label_dropout       = 0,            # Dropout probability of class labels for classifier-free guidance.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.label_dropout = label_dropout\n",
    "        emb_channels = model_channels * channel_mult_emb\n",
    "        init = dict(init_mode='kaiming_uniform', init_weight=np.sqrt(1/3), init_bias=np.sqrt(1/3))\n",
    "        init_zero = dict(init_mode='kaiming_uniform', init_weight=0, init_bias=0)\n",
    "        block_kwargs = dict(emb_channels=emb_channels, channels_per_head=64, dropout=dropout, init=init, init_zero=init_zero)\n",
    "\n",
    "        # Mapping.\n",
    "        self.map_noise = PositionalEmbedding(num_channels=model_channels)\n",
    "        self.map_augment = Linear(in_features=augment_dim, out_features=model_channels, bias=False, **init_zero) if augment_dim else None\n",
    "        self.map_layer0 = Linear(in_features=model_channels, out_features=emb_channels, **init)\n",
    "        self.map_layer1 = Linear(in_features=emb_channels, out_features=emb_channels, **init)\n",
    "        self.map_label = Linear(in_features=label_dim, out_features=emb_channels, bias=False, init_mode='kaiming_normal', init_weight=np.sqrt(label_dim)) if label_dim else None\n",
    "\n",
    "        # Encoder.\n",
    "        self.enc = torch.nn.ModuleDict()\n",
    "        cout = in_channels\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            res = img_resolution >> level\n",
    "            if level == 0:\n",
    "                cin = cout\n",
    "                cout = model_channels * mult\n",
    "                self.enc[f'{res}x{res}_conv'] = Conv2d(in_channels=cin, out_channels=cout, kernel=3, **init)\n",
    "            else:\n",
    "                self.enc[f'{res}x{res}_down'] = UNetBlock(in_channels=cout, out_channels=cout, down=True, **block_kwargs)\n",
    "            for idx in range(num_blocks):\n",
    "                cin = cout\n",
    "                cout = model_channels * mult\n",
    "                self.enc[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=(res in attn_resolutions), **block_kwargs)\n",
    "        skips = [block.out_channels for block in self.enc.values()]\n",
    "\n",
    "        # Decoder.\n",
    "        self.dec = torch.nn.ModuleDict()\n",
    "        for level, mult in reversed(list(enumerate(channel_mult))):\n",
    "            res = img_resolution >> level\n",
    "            if level == len(channel_mult) - 1:\n",
    "                self.dec[f'{res}x{res}_in0'] = UNetBlock(in_channels=cout, out_channels=cout, attention=True, **block_kwargs)\n",
    "                self.dec[f'{res}x{res}_in1'] = UNetBlock(in_channels=cout, out_channels=cout, **block_kwargs)\n",
    "            else:\n",
    "                self.dec[f'{res}x{res}_up'] = UNetBlock(in_channels=cout, out_channels=cout, up=True, **block_kwargs)\n",
    "            for idx in range(num_blocks + 1):\n",
    "                cin = cout + skips.pop()\n",
    "                cout = model_channels * mult\n",
    "                self.dec[f'{res}x{res}_block{idx}'] = UNetBlock(in_channels=cin, out_channels=cout, attention=(res in attn_resolutions), **block_kwargs)\n",
    "        self.out_norm = GroupNorm(num_channels=cout)\n",
    "        self.out_conv = Conv2d(in_channels=cout, out_channels=out_channels, kernel=3, **init_zero)\n",
    "\n",
    "    def forward(self, x, noise_labels, class_labels, augment_labels=None):\n",
    "        # Mapping.\n",
    "        emb = self.map_noise(noise_labels)\n",
    "        if self.map_augment is not None and augment_labels is not None:\n",
    "            emb = emb + self.map_augment(augment_labels)\n",
    "        emb = silu(self.map_layer0(emb))\n",
    "        emb = self.map_layer1(emb)\n",
    "        if self.map_label is not None:\n",
    "            tmp = class_labels\n",
    "            if self.training and self.label_dropout:\n",
    "                tmp = tmp * (torch.rand([x.shape[0], 1], device=x.device) >= self.label_dropout).to(tmp.dtype)\n",
    "            emb = emb + self.map_label(tmp)\n",
    "        emb = silu(emb)\n",
    "\n",
    "        # Encoder.\n",
    "        skips = []\n",
    "        for block in self.enc.values():\n",
    "            x = block(x, emb) if isinstance(block, UNetBlock) else block(x)\n",
    "            skips.append(x)\n",
    "\n",
    "        # Decoder.\n",
    "        for block in self.dec.values():\n",
    "            if x.shape[1] != block.in_channels:\n",
    "                x = torch.cat([x, skips.pop()], dim=1)\n",
    "            x = block(x, emb)\n",
    "        x = self.out_conv(silu(self.out_norm(x)))\n",
    "        return x\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Preconditioning corresponding to the variance preserving (VP) formulation\n",
    "# from the paper \"Score-Based Generative Modeling through Stochastic\n",
    "# Differential Equations\".\n",
    "\n",
    "# @persistence.persistent_class\n",
    "class VPPrecond(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        img_resolution,                 # Image resolution.\n",
    "        img_channels,                   # Number of color channels.\n",
    "        label_dim       = 0,            # Number of class labels, 0 = unconditional.\n",
    "        use_fp16        = False,        # Execute the underlying model at FP16 precision?\n",
    "        beta_d          = 19.9,         # Extent of the noise level schedule.\n",
    "        beta_min        = 0.1,          # Initial slope of the noise level schedule.\n",
    "        M               = 1000,         # Original number of timesteps in the DDPM formulation.\n",
    "        epsilon_t       = 1e-5,         # Minimum t-value used during training.\n",
    "        model_type      = 'SongUNet',   # Class name of the underlying model.\n",
    "        **model_kwargs,                 # Keyword arguments for the underlying model.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_resolution = img_resolution\n",
    "        self.img_channels = img_channels\n",
    "        self.label_dim = label_dim\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.beta_d = beta_d\n",
    "        self.beta_min = beta_min\n",
    "        self.M = M\n",
    "        self.epsilon_t = epsilon_t\n",
    "        self.sigma_min = float(self.sigma(epsilon_t))\n",
    "        self.sigma_max = float(self.sigma(1))\n",
    "        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels, label_dim=label_dim, **model_kwargs)\n",
    "\n",
    "    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n",
    "        x = x.to(torch.float32)\n",
    "        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n",
    "        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n",
    "        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n",
    "\n",
    "        c_skip = 1\n",
    "        c_out = -sigma\n",
    "        c_in = 1 / (sigma ** 2 + 1).sqrt()\n",
    "        c_noise = (self.M - 1) * self.sigma_inv(sigma)\n",
    "\n",
    "        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n",
    "        assert F_x.dtype == dtype\n",
    "        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n",
    "        return D_x\n",
    "\n",
    "    def sigma(self, t):\n",
    "        t = torch.as_tensor(t)\n",
    "        return ((0.5 * self.beta_d * (t ** 2) + self.beta_min * t).exp() - 1).sqrt()\n",
    "\n",
    "    def sigma_inv(self, sigma):\n",
    "        sigma = torch.as_tensor(sigma)\n",
    "        return ((self.beta_min ** 2 + 2 * self.beta_d * (1 + sigma ** 2).log()).sqrt() - self.beta_min) / self.beta_d\n",
    "\n",
    "    def round_sigma(self, sigma):\n",
    "        return torch.as_tensor(sigma)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Preconditioning corresponding to the variance exploding (VE) formulation\n",
    "# from the paper \"Score-Based Generative Modeling through Stochastic\n",
    "# Differential Equations\".\n",
    "\n",
    "# @persistence.persistent_class\n",
    "class VEPrecond(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        img_resolution,                 # Image resolution.\n",
    "        img_channels,                   # Number of color channels.\n",
    "        label_dim       = 0,            # Number of class labels, 0 = unconditional.\n",
    "        use_fp16        = False,        # Execute the underlying model at FP16 precision?\n",
    "        sigma_min       = 0.02,         # Minimum supported noise level.\n",
    "        sigma_max       = 100,          # Maximum supported noise level.\n",
    "        model_type      = 'SongUNet',   # Class name of the underlying model.\n",
    "        **model_kwargs,                 # Keyword arguments for the underlying model.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_resolution = img_resolution\n",
    "        self.img_channels = img_channels\n",
    "        self.label_dim = label_dim\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_max = sigma_max\n",
    "        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels, label_dim=label_dim, **model_kwargs)\n",
    "\n",
    "    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n",
    "        x = x.to(torch.float32)\n",
    "        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n",
    "        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n",
    "        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n",
    "\n",
    "        c_skip = 1\n",
    "        c_out = sigma\n",
    "        c_in = 1\n",
    "        c_noise = (0.5 * sigma).log()\n",
    "\n",
    "        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n",
    "        assert F_x.dtype == dtype\n",
    "        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n",
    "        return D_x\n",
    "\n",
    "    def round_sigma(self, sigma):\n",
    "        return torch.as_tensor(sigma)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Preconditioning corresponding to improved DDPM (iDDPM) formulation from\n",
    "# the paper \"Improved Denoising Diffusion Probabilistic Models\".\n",
    "\n",
    "# @persistence.persistent_class\n",
    "class iDDPMPrecond(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        img_resolution,                     # Image resolution.\n",
    "        img_channels,                       # Number of color channels.\n",
    "        label_dim       = 0,                # Number of class labels, 0 = unconditional.\n",
    "        use_fp16        = False,            # Execute the underlying model at FP16 precision?\n",
    "        C_1             = 0.001,            # Timestep adjustment at low noise levels.\n",
    "        C_2             = 0.008,            # Timestep adjustment at high noise levels.\n",
    "        M               = 1000,             # Original number of timesteps in the DDPM formulation.\n",
    "        model_type      = 'DhariwalUNet',   # Class name of the underlying model.\n",
    "        **model_kwargs,                     # Keyword arguments for the underlying model.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_resolution = img_resolution\n",
    "        self.img_channels = img_channels\n",
    "        self.label_dim = label_dim\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.C_1 = C_1\n",
    "        self.C_2 = C_2\n",
    "        self.M = M\n",
    "        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels*2, label_dim=label_dim, **model_kwargs)\n",
    "\n",
    "        u = torch.zeros(M + 1)\n",
    "        for j in range(M, 0, -1): # M, ..., 1\n",
    "            u[j - 1] = ((u[j] ** 2 + 1) / (self.alpha_bar(j - 1) / self.alpha_bar(j)).clip(min=C_1) - 1).sqrt()\n",
    "        self.register_buffer('u', u)\n",
    "        self.sigma_min = float(u[M - 1])\n",
    "        self.sigma_max = float(u[0])\n",
    "\n",
    "    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n",
    "        x = x.to(torch.float32)\n",
    "        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n",
    "        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n",
    "        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n",
    "\n",
    "        c_skip = 1\n",
    "        c_out = -sigma\n",
    "        c_in = 1 / (sigma ** 2 + 1).sqrt()\n",
    "        c_noise = self.M - 1 - self.round_sigma(sigma, return_index=True).to(torch.float32)\n",
    "\n",
    "        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n",
    "        assert F_x.dtype == dtype\n",
    "        D_x = c_skip * x + c_out * F_x[:, :self.img_channels].to(torch.float32)\n",
    "        return D_x\n",
    "\n",
    "    def alpha_bar(self, j):\n",
    "        j = torch.as_tensor(j)\n",
    "        return (0.5 * np.pi * j / self.M / (self.C_2 + 1)).sin() ** 2\n",
    "\n",
    "    def round_sigma(self, sigma, return_index=False):\n",
    "        sigma = torch.as_tensor(sigma)\n",
    "        index = torch.cdist(sigma.to(self.u.device).to(torch.float32).reshape(1, -1, 1), self.u.reshape(1, -1, 1)).argmin(2)\n",
    "        result = index if return_index else self.u[index.flatten()].to(sigma.dtype)\n",
    "        return result.reshape(sigma.shape).to(sigma.device)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# Improved preconditioning proposed in the paper \"Elucidating the Design\n",
    "# Space of Diffusion-Based Generative Models\" (EDM).\n",
    "\n",
    "# @persistence.persistent_class\n",
    "class EDMPrecond(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "        img_resolution,                     # Image resolution.\n",
    "        img_channels,                       # Number of color channels.\n",
    "        label_dim       = 0,                # Number of class labels, 0 = unconditional.\n",
    "        use_fp16        = False,            # Execute the underlying model at FP16 precision?\n",
    "        sigma_min       = 0,                # Minimum supported noise level.\n",
    "        sigma_max       = float('inf'),     # Maximum supported noise level.\n",
    "        sigma_data      = 0.5,              # Expected standard deviation of the training data.\n",
    "        model_type      = 'DhariwalUNet',   # Class name of the underlying model.\n",
    "        **model_kwargs,                     # Keyword arguments for the underlying model.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.img_resolution = img_resolution\n",
    "        self.img_channels = img_channels\n",
    "        self.label_dim = label_dim\n",
    "        self.use_fp16 = use_fp16\n",
    "        self.sigma_min = sigma_min\n",
    "        self.sigma_max = sigma_max\n",
    "        self.sigma_data = sigma_data\n",
    "        self.model = globals()[model_type](img_resolution=img_resolution, in_channels=img_channels, out_channels=img_channels, label_dim=label_dim, **model_kwargs)\n",
    "\n",
    "    def forward(self, x, sigma, class_labels=None, force_fp32=False, **model_kwargs):\n",
    "        x = x.to(torch.float32)\n",
    "        sigma = sigma.to(torch.float32).reshape(-1, 1, 1, 1)\n",
    "        class_labels = None if self.label_dim == 0 else torch.zeros([1, self.label_dim], device=x.device) if class_labels is None else class_labels.to(torch.float32).reshape(-1, self.label_dim)\n",
    "        dtype = torch.float16 if (self.use_fp16 and not force_fp32 and x.device.type == 'cuda') else torch.float32\n",
    "\n",
    "        c_skip = self.sigma_data ** 2 / (sigma ** 2 + self.sigma_data ** 2)\n",
    "        c_out = sigma * self.sigma_data / (sigma ** 2 + self.sigma_data ** 2).sqrt()\n",
    "        c_in = 1 / (self.sigma_data ** 2 + sigma ** 2).sqrt()\n",
    "        c_noise = sigma.log() / 4\n",
    "\n",
    "        F_x = self.model((c_in * x).to(dtype), c_noise.flatten(), class_labels=class_labels, **model_kwargs)\n",
    "        assert F_x.dtype == dtype\n",
    "        D_x = c_skip * x + c_out * F_x.to(torch.float32)\n",
    "        return D_x\n",
    "\n",
    "    def round_sigma(self, sigma):\n",
    "        return torch.as_tensor(sigma)\n",
    "\n",
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers import UNet2DModel\n",
    "\n",
    "class ClassConditionedUnet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # The embedding layer will map the class label to a scalar value per pixel (like x)\n",
    "        self.class_emb = nn.Embedding(num_classes, 28 * 28)  # Output size matches spatial dimensions\n",
    "\n",
    "        # Self.model is an unconditional UNet with extra input channels to accept the conditioning information\n",
    "        self.model = UNet2DModel(\n",
    "            sample_size=28,           # the target image resolution\n",
    "            in_channels=2,            # Additional input channel for class conditioning\n",
    "            out_channels=1,           # the number of output channels\n",
    "            layers_per_block=2,       # how many ResNet layers to use per UNet block\n",
    "            block_out_channels=(32, 64, 64), \n",
    "            down_block_types=( \n",
    "                \"DownBlock2D\",        # a regular ResNet downsampling block\n",
    "                \"AttnDownBlock2D\",    # a ResNet downsampling block with spatial self-attention\n",
    "                \"AttnDownBlock2D\",\n",
    "            ), \n",
    "            up_block_types=(\n",
    "                \"AttnUpBlock2D\", \n",
    "                \"AttnUpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n",
    "                \"UpBlock2D\",          # a regular ResNet upsampling block\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Our forward method now takes the class labels as an additional argument\n",
    "    def forward(self, x, t, class_labels):\n",
    "        # Shape of x:\n",
    "        bs, ch, w, h = x.shape\n",
    "        \n",
    "        # class conditioning is mapped to (bs, 1, 28, 28)\n",
    "        class_cond = self.class_emb(class_labels)  # (bs, 28*28)\n",
    "        class_cond = class_cond.view(bs, 1, 28, 28)  # Reshape to match input x\n",
    "        \n",
    "        # Net input is now x and class cond concatenated together along dimension 1\n",
    "        net_input = torch.cat((x, class_cond), 1)  # (bs, 2, 28, 28)\n",
    "\n",
    "        # Feed this to the UNet alongside the timestep and return the prediction\n",
    "        return self.model(net_input, t).sample  # (bs, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many runs through the data should we do?\n",
    "n_epochs = 10\n",
    "\n",
    "# Our network \n",
    "net = ClassConditionedUnet().to(device)\n",
    "\n",
    "# Our loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# The optimizer\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3) \n",
    "\n",
    "# Keeping a record of the losses for later viewing\n",
    "losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class ConditionalSongUNet(nn.Module):\n",
    "    def __init__(self, img_resolution=32, in_channels=1, out_channels=1, label_dim=10, model_channels=128):\n",
    "        super().__init__()\n",
    "        self.model = SongUNet(\n",
    "            img_resolution=img_resolution,\n",
    "            in_channels=in_channels + 1,  # Extra channel for class conditioning\n",
    "            out_channels=out_channels,\n",
    "            label_dim=label_dim,\n",
    "            model_channels=model_channels\n",
    "        )\n",
    "        self.class_embedding = nn.Embedding(label_dim, img_resolution * img_resolution)\n",
    "    \n",
    "    def forward(self, x, timesteps, labels):\n",
    "        class_emb = self.class_embedding(labels).view(x.shape[0], 1, x.shape[2], x.shape[3]).to(device)\n",
    "        x_cond = torch.cat((x, class_emb), dim=1)  # Concatenate along channel dimension\n",
    "        return self.model(x_cond, timesteps, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scheduler\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many runs through the data should we do?\n",
    "n_epochs = 10\n",
    "\n",
    "# Initialize model and optimizer\n",
    "net = ConditionalSongUNet().to(device)\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Keeping a record of the losses for later viewing\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:06<00:00, 1531936.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/MNIST/raw/train-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 965740.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/MNIST/raw/train-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 6294830.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4813170.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist/MNIST/raw\n",
      "\n",
      "Input shape: torch.Size([8, 1, 28, 28])\n",
      "Labels: tensor([2, 1, 4, 2, 0, 1, 0, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "dataset = torchvision.datasets.MNIST(root=\"mnist/\", train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "# Feed it into a dataloader (batch size 8 here just for demo)\n",
    "train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# View some examples\n",
    "x, y = next(iter(train_dataloader))\n",
    "print('Input shape:', x.shape)\n",
    "print('Labels:', y)\n",
    "plt.imshow(torchvision.utils.make_grid(x)[0], cmap='Greys');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassConditionedUnet(nn.Module):\n",
    "  def __init__(self, num_classes=10, class_emb_size=4):\n",
    "    super().__init__()\n",
    "    \n",
    "    # The embedding layer will map the class label to a vector of size class_emb_size\n",
    "    self.class_emb = nn.Embedding(num_classes, class_emb_size)\n",
    "\n",
    "    # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)\n",
    "    self.model = UNet2DModel(\n",
    "        sample_size=28,           # the target image resolution\n",
    "        in_channels=1 + class_emb_size, # Additional input channels for class cond.\n",
    "        out_channels=1,           # the number of output channels\n",
    "        layers_per_block=2,       # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(32, 64, 64), \n",
    "        down_block_types=( \n",
    "            \"DownBlock2D\",        # a regular ResNet downsampling block\n",
    "            \"AttnDownBlock2D\",    # a ResNet downsampling block with spatial self-attention\n",
    "            \"AttnDownBlock2D\",\n",
    "        ), \n",
    "        up_block_types=(\n",
    "            \"AttnUpBlock2D\", \n",
    "            \"AttnUpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",          # a regular ResNet upsampling block\n",
    "          ),\n",
    "    )\n",
    "\n",
    "  # Our forward method now takes the class labels as an additional argument\n",
    "  def forward(self, x, t, class_labels):\n",
    "    # Shape of x:\n",
    "    bs, ch, w, h = x.shape\n",
    "    \n",
    "    # class conditioning in right shape to add as additional input channels\n",
    "    class_cond = self.class_emb(class_labels) # Map to embedding dimension\n",
    "    class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)\n",
    "    # x is shape (bs, 1, 28, 28) and class_cond is now (bs, 4, 28, 28)\n",
    "\n",
    "    # Net input is now x and class cond concatenated together along dimension 1\n",
    "    net_input = torch.cat((x, class_cond), 1) # (bs, 5, 28, 28)\n",
    "\n",
    "    # Feed this to the UNet alongside the timestep and return the prediction\n",
    "    return self.model(net_input, t).sample # (bs, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0. Average of the last 100 loss values: 0.052965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 14.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 1. Average of the last 100 loss values: 0.046416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 14.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 2. Average of the last 100 loss values: 0.044094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 14.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 3. Average of the last 100 loss values: 0.042591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 14.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 4. Average of the last 100 loss values: 0.041436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 13.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 5. Average of the last 100 loss values: 0.041199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 6. Average of the last 100 loss values: 0.040251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 7. Average of the last 100 loss values: 0.039607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 13.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 8. Average of the last 100 loss values: 0.038799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 469/469 [00:33<00:00, 13.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 9. Average of the last 100 loss values: 0.039131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7677703204f0>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Redefining the dataloader to set the batch size higher than the demo of 8\n",
    "train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# How many runs through the data should we do?\n",
    "n_epochs = 10\n",
    "\n",
    "# Our network \n",
    "net = ClassConditionedUnet().to(device)\n",
    "\n",
    "# Our loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# The optimizer\n",
    "opt = torch.optim.Adam(net.parameters(), lr=1e-3) \n",
    "\n",
    "# Keeping a record of the losses for later viewing\n",
    "losses = []\n",
    "\n",
    "# The training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for x, y in tqdm(train_dataloader):\n",
    "        \n",
    "        # Get some data and prepare the corrupted version\n",
    "        x = x.to(device) * 2 - 1 # Data on the GPU (mapped to (-1, 1))\n",
    "        y = y.to(device)\n",
    "        noise = torch.randn_like(x)\n",
    "        timesteps = torch.randint(0, 999, (x.shape[0],)).long().to(device)\n",
    "        noisy_x = noise_scheduler.add_noise(x, noise, timesteps)\n",
    "\n",
    "        # Get the model prediction\n",
    "        pred = net(noisy_x, timesteps, y) # Note that we pass in the labels y\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(pred, noise) # How close is the output to the noise\n",
    "\n",
    "        # Backprop and update the params:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Store the loss for later\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Print out the average of the last 100 loss values to get an idea of progress:\n",
    "    avg_loss = sum(losses[-100:])/100\n",
    "    print(f'Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}')\n",
    "\n",
    "# View the loss curve\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@markdown Sampling some different digits:\n",
    "\n",
    "# Prepare random x to start from, plus some desired labels y\n",
    "x = torch.randn(80, 1, 28, 28).to(device)\n",
    "y = torch.tensor([[i]*8 for i in range(10)]).flatten().to(device)\n",
    "\n",
    "# Sampling loop\n",
    "for i, t in tqdm(enumerate(noise_scheduler.timesteps)):\n",
    "\n",
    "    # Get model pred\n",
    "    with torch.no_grad():\n",
    "        residual = net(x, t, y)  # Again, note that we pass in our labels y\n",
    "\n",
    "    # Update sample with step\n",
    "    x = noise_scheduler.step(residual, t, x).prev_sample\n",
    "\n",
    "# Show the results\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "ax.imshow(torchvision.utils.make_grid(x.detach().cpu().clip(-1, 1), nrow=8)[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now break the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_emb_size = 1\n",
    "model = UNet2DModel(\n",
    "        sample_size=28,           # the target image resolution\n",
    "        in_channels=1 + class_emb_size, # Additional input channels for class cond.\n",
    "        out_channels=1,           # the number of output channels\n",
    "        layers_per_block=2,       # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(32, 64, 64), \n",
    "        down_block_types=( \n",
    "            \"DownBlock2D\",        # a regular ResNet downsampling block\n",
    "            \"AttnDownBlock2D\",    # a ResNet downsampling block with spatial self-attention\n",
    "            \"AttnDownBlock2D\",\n",
    "        ), \n",
    "        up_block_types=(\n",
    "            \"AttnUpBlock2D\", \n",
    "            \"AttnUpBlock2D\",      # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",          # a regular ResNet upsampling block\n",
    "          ),\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ncDataset(x_train_patches, y_train_patches)\n",
    "val_dataset = ncDataset(x_val_patches, y_val_patches)\n",
    "test_dataset = ncDataset(x_val_patches, y_val_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=20, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input.shape torch.Size([20, 1, 32, 32])\n",
      "Targets.shape torch.Size([20, 1, 32, 32])\n",
      "Noise.shape torch.Size([20, 1, 32, 32])\n",
      "Timesteps.shape, Timesteps: torch.Size([20]) tensor([ 97, 975, 846, 116, 998, 511, 199, 737, 959,   6,  85, 149, 344, 915,\n",
      "        527,  74, 204, 196, 413, 500], device='cuda:0')\n",
      "Noisy Input shape:  torch.Size([20, 1, 32, 32])\n",
      "Net input shape: torch.Size([20, 2, 32, 32])\n",
      "Pred shape: torch.Size([20, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    input, targets = batch\n",
    "    print('Input.shape', input.size())  # Should print torch.Size([16, 1, 30, 30])\n",
    "    print('Targets.shape', targets.size())  # Should print torch.Size([16, 1, 30, 601])\n",
    "    x = input.to(device)  # Data on the GPU (mapped to (-1, 1))\n",
    "    y = targets.to(device)\n",
    "    noise = torch.randn_like(x)\n",
    "    print('Noise.shape', noise.shape)\n",
    "    timesteps = torch.randint(0, 999, (x.shape[0],)).long().to(device)\n",
    "    print('Timesteps.shape, Timesteps:', timesteps.shape, timesteps)\n",
    "    noisy_x = noise_scheduler.add_noise(x, noise, timesteps)\n",
    "    print('Noisy Input shape: ',noisy_x.shape)\n",
    "    bs, ch, w, h = x.shape\n",
    "    net_input = torch.cat((noisy_x, y), 1) # (bs, 2, 28, 28)\n",
    "    print('Net input shape:', net_input.shape)\n",
    "    pred = model(net_input, timesteps).sample\n",
    "    print('Pred shape:', pred.shape)\n",
    "# Calculate the loss\n",
    "    loss = loss_fn(pred, noise) # How close is the output to the noise\n",
    "\n",
    "    # Backprop and update the params:\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # Store the loss for later\n",
    "    losses.append(loss.item())\n",
    "\n",
    "# # Print out the average of the last 100 loss values to get an idea of progress:\n",
    "# avg_loss = sum(losses[-100:])/100\n",
    "# print(f'Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}')\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It is important to take guidance of the hyperparameters from the papers that have performed well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
